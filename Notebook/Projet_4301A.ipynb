{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c90521d3",
   "metadata": {},
   "source": [
    "# SCRIPT 1 - STACKING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7854a2fa",
   "metadata": {},
   "source": [
    "Toutes les explications de ce script ainsi que des deux autres sont disponibles dans mon rapport"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e719f73c",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe1ceac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler, StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV, cross_val_predict, RandomizedSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV, ElasticNet, ElasticNetCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingClassifier, VotingClassifier, StackingClassifier\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.base import clone\n",
    "from sklearn.tree import DecisionTreeClassifier,plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "# explicitly require this experimental feature\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "# now you can import normally from sklearn.impute\n",
    "from sklearn.impute import IterativeImputer, SimpleImputer\n",
    "\n",
    "#évaluer la performance des modèles\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "#hyperparamétrisation\n",
    "from sklearn.model_selection import GridSearchCV,cross_val_score\n",
    "\n",
    "#visualisation d'un arbre\n",
    "from sklearn.tree import export_graphviz,export_text\n",
    "from subprocess import call\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.feature_selection import SequentialFeatureSelector, SelectKBest, f_classif, SelectFromModel, chi2, RFECV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import metrics   \n",
    "import matplotlib.pylab as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import lightgbm as lgb\n",
    "from mlxtend.classifier import StackingCVClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9643c178",
   "metadata": {},
   "source": [
    "# Script a lancé (Assurance Vie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b89857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on read le csv\n",
    "data = pd.read_csv(\"train.csv\")\n",
    "data=data.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "\n",
    "#définition des types de variables\n",
    "nominal = ['Nat', 'Statmat', 'Sexe', 'Couple', 'Occ', 'Statpro', 'Herit', 'Pere', 'Mere', 'Gpp', 'Gpm', 'Evtgrav', 'Livep', \n",
    "           'Eplog', 'Pep', 'Vmob', 'Assdec', 'Livdf', 'Pel', 'Cel', 'Capi', 'Epsal', 'Pea', 'Zres', 'Logt', 'Terre', \n",
    "           'Dette', 'Detlog', 'Detvo', 'Dip', 'Work', 'Urbani'] \n",
    "           \n",
    "ordinal = []\n",
    "           \n",
    "discrete = ['Nbenf', 'Age']\n",
    "\n",
    "# Standardisation\n",
    "data[discrete] = StandardScaler().fit_transform(data[discrete])\n",
    "\n",
    "# Remplacement des valeurs manquantes\n",
    "\n",
    "data[nominal] = data[nominal].apply(lambda series: pd.Series(\n",
    "        LabelEncoder().fit_transform(series[series.notnull()]),\n",
    "        index=series[series.notnull()].index\n",
    "    ))\n",
    "imp_cat = IterativeImputer(estimator=GradientBoostingClassifier(random_state=47), \n",
    "                               initial_strategy='most_frequent',\n",
    "                               max_iter=10, random_state=47, verbose=2)\n",
    "data[nominal] = imp_cat.fit_transform(data[nominal])\n",
    "\n",
    "#Split\n",
    "y = data['Assvie'] ==\"O\"\n",
    "X=data.drop(['Assvie', 'Retraite'], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "#Features selection\n",
    "selector = RFECV(GradientBoostingClassifier(random_state=47, max_features='sqrt'), step=4, scoring=\"roc_auc\")\n",
    "selector.fit(X,y)\n",
    "X_train_selected = selector.transform(X_train)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "#Logistic Regression \n",
    "lr=LogisticRegression(C= 0.1, max_iter= 1000, penalty= 'l2', solver='lbfgs', random_state=42)\n",
    "# train\n",
    "lr.fit(X_train_selected, y_train)\n",
    "\n",
    "#KNeighborsClassifier\n",
    "knn=KNeighborsClassifier(n_neighbors= 50, weights= 'distance')\n",
    "# train\n",
    "knn.fit(X_train_selected, y_train)\n",
    "\n",
    "#Random Forest Classifier \n",
    "rfc = RandomForestClassifier(criterion= 'gini', max_depth= 12, max_features= 2, n_estimators= 500, random_state=46)\n",
    "# train\n",
    "rfc.fit(X_train_selected, y_train)\n",
    "\n",
    "#Gradient Boosting Classifier\n",
    "boost=GradientBoostingClassifier(max_features=0.3, n_iter_no_change=26, random_state=12, subsample=0.5,n_estimators=177,max_depth=2, learning_rate=0.1)\n",
    "\n",
    "# train\n",
    "boost.fit(X_train_selected, y_train)\n",
    "\n",
    "#XGBoostClassifier\n",
    "xgb=XGBClassifier(max_depth=4, learning_rate=0.1, colsample_bytree= 0.6, random_state=29, subsample=0.7, n_estimators=90, min_child_weight=8, eval_metric='auc', gamma=0.1)\n",
    "\n",
    "# train\n",
    "xgb.fit(X_train_selected, y_train)\n",
    "\n",
    "#LGBMClassifier\n",
    "lgbc=lgb.LGBMClassifier(max_depth=4, learning_rate=0.1, min_split_gain=0.3, num_leaves=10,reg_alpha= 1.2, reg_lambda= 1.2,\n",
    "                           random_state=9, subsample=0.8, n_estimators=177, colsample_bytree= 0.8, subsample_freq= 10)\n",
    "\n",
    "# train\n",
    "lgbc.fit(X_train_selected, y_train)\n",
    "\n",
    "#Stacking\n",
    "best_models_auc=[\n",
    "    ('Boosting', boost),\n",
    "    ('Random Forest', rfc),\n",
    "    ('Logistic Regression',lr),\n",
    "    ('KNeighbours', knn),\n",
    "    ('XGBoost', xgb),\n",
    "    ('LGBoost', lgbc)\n",
    "]\n",
    "\n",
    "# ensemble = base models + meta-learner\n",
    "stacking = StackingClassifier(best_models_auc, cv=10, final_estimator=LogisticRegression())\n",
    "\n",
    "# train\n",
    "stacking.fit(X_train_selected, y_train)\n",
    "\n",
    "# test\n",
    "#score = stacking.score(X_test, y_test)\n",
    "score=roc_auc_score(y_test,stacking.predict_proba(X_test_selected)[:,1])\n",
    "\n",
    "# show\n",
    "print('K-fold stacking: {:.4f}'.format(score))\n",
    "\n",
    "#################################################################################################################\n",
    "#Test\n",
    "test=pd.read_csv(\"test.csv\")\n",
    "test=test.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "test[discrete] = StandardScaler().fit_transform(test[discrete])\n",
    "\n",
    "test[nominal] = test[nominal].apply(lambda series: pd.Series(\n",
    "        LabelEncoder().fit_transform(series[series.notnull()]),\n",
    "        index=series[series.notnull()].index\n",
    "    ))\n",
    "imp_cat = IterativeImputer(estimator=GradientBoostingClassifier(random_state=47), \n",
    "                               initial_strategy='most_frequent',\n",
    "                               max_iter=10, random_state=47, verbose=2)\n",
    "test[nominal] = imp_cat.fit_transform(test[nominal])\n",
    "\n",
    "test2=selector.transform(test)\n",
    "\n",
    "#Création des probas\n",
    "proba_assvie=stacking.predict_proba(test2)\n",
    "Assvie=[]\n",
    "for i in range(len(proba_assvie)):\n",
    "    Assvie.append(proba_assvie[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff0f137",
   "metadata": {},
   "source": [
    "# Script a lancé (Retraite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b174cdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split\n",
    "y = data['Retraite'] ==\"O\"\n",
    "X=data.drop(['Assvie', 'Retraite'], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "#Features Selection\n",
    "selector = RFECV(GradientBoostingClassifier(random_state=47, max_features='sqrt'), step=3, scoring=\"roc_auc\")\n",
    "selector.fit(X,y)\n",
    "X_train_selected = selector.transform(X_train)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "#Logistic Regression \n",
    "lr=LogisticRegression(C= 1.0, max_iter= 70, penalty= 'l2', solver= 'liblinear', random_state=42)\n",
    "# train\n",
    "lr.fit(X_train_selected, y_train)\n",
    "\n",
    "#KNeighborsClassifier\n",
    "knn=KNeighborsClassifier(n_neighbors= 71, weights= 'distance')\n",
    "# train\n",
    "knn.fit(X_train_selected, y_train)\n",
    "\n",
    "#Random Forest Classifier \n",
    "rfc = RandomForestClassifier(criterion= 'entropy', max_depth= 8, max_features= 4, n_estimators= 200, random_state=42)\n",
    "# train\n",
    "rfc.fit(X_train_selected, y_train)\n",
    "\n",
    "#Gradient Boosting Classifier\n",
    "boost=GradientBoostingClassifier(max_features=0.3, n_iter_no_change=24, learning_rate=0.1, random_state=163, subsample=0.5, n_estimators=500, max_depth=2)\n",
    "\n",
    "# train\n",
    "boost.fit(X_train_selected, y_train)\n",
    "\n",
    "#XGBoostClassifier\n",
    "xgb=XGBClassifier(max_depth=2, learning_rate=0.05, colsample_bytree= 0.6, random_state=124, subsample=0.9, n_estimators=500, min_child_weight=4, eval_metric='auc', gamma=0.1, reg_alpha= 1)\n",
    "\n",
    "# train\n",
    "xgb.fit(X_train_selected, y_train)\n",
    "\n",
    "#LGBMClassifier\n",
    "lgbc=lgb.LGBMClassifier(colsample_bytree= 0.7, random_state=47, learning_rate= 0.1, max_depth= 4, min_split_gain= 0.3, n_estimators= 500, num_leaves= 20, reg_alpha= 1.2, reg_lambda= 1.2, subsample= 0.8, subsample_freq= 20)\n",
    "\n",
    "# train\n",
    "lgbc.fit(X_train_selected, y_train)\n",
    "\n",
    "#Stacking\n",
    "best_models_auc=[\n",
    "    ('Boosting', boost),\n",
    "    ('Random Forest', rfc),\n",
    "    ('Logistic Regression',lr),\n",
    "    ('KNeighbours', knn),\n",
    "    ('XGBoost', xgb),\n",
    "    ('LGBoost', lgbc)\n",
    "]\n",
    "\n",
    "# ensemble = base models + meta-learner\n",
    "stacking = StackingClassifier(best_models_auc, cv=10, final_estimator=LogisticRegression())\n",
    "\n",
    "# train\n",
    "stacking.fit(X_train_selected, y_train)\n",
    "\n",
    "# test\n",
    "#score = stacking.score(X_test, y_test)\n",
    "score=roc_auc_score(y_test,stacking.predict_proba(X_test_selected)[:,1])\n",
    "\n",
    "# show\n",
    "print('K-fold stacking: {:.4f}'.format(score))\n",
    "\n",
    "test3=selector.transform(test)\n",
    "\n",
    "#Création des probas\n",
    "proba_retraite=stacking.predict_proba(test3)\n",
    "retraite=[]\n",
    "for i in range(len(proba_retraite)):\n",
    "    retraite.append(proba_retraite[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1e34a0",
   "metadata": {},
   "source": [
    "# Script a lancé (création du csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ed74b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "id = list(map(lambda x: x, range(0, len(test))))\n",
    "df=pd.DataFrame({\"Id\":id, \"Assvie\":Assvie, \"Retraite\":retraite})\n",
    "df.to_csv(\"script1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef57cfd5",
   "metadata": {},
   "source": [
    "# Détails avec les grid search (à ne pas lancé)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f5b093",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eafc97b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yanis\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_base.py:49: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode = stats.mode(array)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IterativeImputer] Completing matrix with shape (10906, 32)\n",
      "[IterativeImputer] Ending imputation round 1/10, elapsed time 55.24\n",
      "[IterativeImputer] Change: 10.0, scaled tolerance: 0.007 \n",
      "[IterativeImputer] Ending imputation round 2/10, elapsed time 110.97\n",
      "[IterativeImputer] Change: 9.0, scaled tolerance: 0.007 \n",
      "[IterativeImputer] Ending imputation round 3/10, elapsed time 166.56\n",
      "[IterativeImputer] Change: 9.0, scaled tolerance: 0.007 \n",
      "[IterativeImputer] Ending imputation round 4/10, elapsed time 222.29\n",
      "[IterativeImputer] Change: 7.0, scaled tolerance: 0.007 \n",
      "[IterativeImputer] Ending imputation round 5/10, elapsed time 278.26\n",
      "[IterativeImputer] Change: 7.0, scaled tolerance: 0.007 \n",
      "[IterativeImputer] Ending imputation round 6/10, elapsed time 337.03\n",
      "[IterativeImputer] Change: 7.0, scaled tolerance: 0.007 \n",
      "[IterativeImputer] Ending imputation round 7/10, elapsed time 396.54\n",
      "[IterativeImputer] Change: 7.0, scaled tolerance: 0.007 \n",
      "[IterativeImputer] Ending imputation round 8/10, elapsed time 452.74\n",
      "[IterativeImputer] Change: 7.0, scaled tolerance: 0.007 \n",
      "[IterativeImputer] Ending imputation round 9/10, elapsed time 508.17\n",
      "[IterativeImputer] Change: 7.0, scaled tolerance: 0.007 \n",
      "[IterativeImputer] Ending imputation round 10/10, elapsed time 563.26\n",
      "[IterativeImputer] Change: 7.0, scaled tolerance: 0.007 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yanis\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_iterative.py:699: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#définition des types de variables\n",
    "nominal = ['Nat', 'Statmat', 'Sexe', 'Couple', 'Occ', 'Statpro', 'Herit', 'Pere', 'Mere', 'Gpp', 'Gpm', 'Evtgrav', 'Livep', \n",
    "           'Eplog', 'Pep', 'Vmob', 'Assdec', 'Livdf', 'Pel', 'Cel', 'Capi', 'Epsal', 'Pea', 'Zres', 'Logt', 'Terre', \n",
    "           'Dette', 'Detlog', 'Detvo', 'Dip', 'Work', 'Urbani'] \n",
    "           \n",
    "ordinal = []\n",
    "           \n",
    "discrete = ['Nbenf', 'Age']\n",
    "\n",
    "# Standardisation\n",
    "data[discrete] = StandardScaler().fit_transform(data[discrete])\n",
    "\n",
    "# Remplacement des valeurs manquantes\n",
    "\n",
    "data[nominal] = data[nominal].apply(lambda series: pd.Series(\n",
    "        LabelEncoder().fit_transform(series[series.notnull()]),\n",
    "        index=series[series.notnull()].index\n",
    "    ))\n",
    "imp_cat = IterativeImputer(estimator=GradientBoostingClassifier(random_state=47), \n",
    "                               initial_strategy='most_frequent',\n",
    "                               max_iter=10, random_state=47, verbose=2)\n",
    "data[nominal] = imp_cat.fit_transform(data[nominal])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90df4c61",
   "metadata": {},
   "source": [
    "# Assurance Vie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f403527d",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7da242c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['Assvie'] ==\"O\"\n",
    "X=data.drop(['Assvie', 'Retraite'], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5756a9",
   "metadata": {},
   "source": [
    "## Features Selections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77fba47",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808fb9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_selector = {'estimator':[GradientBoostingClassifier(random_state=47, max_features='sqrt'), RandomForestClassifier(random_state=47, max_features='sqrt')],\n",
    "                  'step' :[3,4,5],\n",
    "                  'scoring' : ['roc_auc']\n",
    "}\n",
    "\n",
    "# model\n",
    "rfecv = RFECV(GradientBoostingClassifier(random_state=47, max_features='sqrt'))\n",
    "\n",
    "# cross-validation strategy\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=47)\n",
    "\n",
    "# hyper-parameter search\n",
    "gridsearch = GridSearchCV(rfecv, param_selector, scoring=\"roc_auc\", cv=cv, verbose=True, n_jobs=-1)\n",
    "gridsearch.fit(X, y)\n",
    "print('Best hyperparameters:\\n' + str(gridsearch.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3082b477",
   "metadata": {},
   "source": [
    "### Selector final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6605022d",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = RFECV(GradientBoostingClassifier(random_state=47, max_features='sqrt'), step=4, scoring=\"roc_auc\")\n",
    "selector.fit(X,y)\n",
    "X_train_selected = selector.transform(X_train)\n",
    "X_test_selected = selector.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1780a0ea",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b910d1b4",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe81919",
   "metadata": {},
   "source": [
    "#### GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c145fc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'penalty':['l1','l2','elasticnet'],\n",
    "              'solver' : ['newton-cg', 'lbfgs', 'liblinear'],\n",
    "               'max_iter':[100,1000], \n",
    "              'C':np.logspace(-3,3,7)}\n",
    "\n",
    "lr=LogisticRegression(random_state=42)\n",
    "\n",
    "cv = KFold(n_splits=10)\n",
    "\n",
    "gslr = GridSearchCV(lr, parameters, cv=cv, scoring=\"roc_auc\", verbose=3, n_jobs=-1)\n",
    "gslr.fit(X_train_selected, y_train)\n",
    "print('---')\n",
    "print('Best hyperparameters:\\n' + str(gslr.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd91fc24",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d9a0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "lr = gslr.best_estimator_\n",
    "\n",
    "# train\n",
    "lr.fit(X_train_selected, y_train)\n",
    "\n",
    "# test\n",
    "score=roc_auc_score(y_test,lr.predict_proba(X_test_selected)[:,1])\n",
    "# print\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f10191",
   "metadata": {},
   "source": [
    "### KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76aa86f2",
   "metadata": {},
   "source": [
    "#### GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4b0f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'n_neighbors':list(range(1, 200)),\n",
    "              'weights':['uniform', 'distance'],\n",
    "             }\n",
    "\n",
    "knn=KNeighborsClassifier()\n",
    "\n",
    "cv = KFold(n_splits=10)\n",
    "\n",
    "gsknn = GridSearchCV(knn, parameters, cv=cv, scoring=\"roc_auc\", verbose=3, n_jobs=-1)\n",
    "gsknn.fit(X_train_selected, y_train)\n",
    "print('---')\n",
    "print('Best hyperparameters:\\n' + str(gsknn.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5d62dc",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4279d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "knn = gsknn.best_estimator_\n",
    "\n",
    "# train\n",
    "knn.fit(X_train_selected, y_train)\n",
    "\n",
    "# test\n",
    "score=roc_auc_score(y_test,knn.predict_proba(X_test_selected)[:,1])\n",
    "# print\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97717dd2",
   "metadata": {},
   "source": [
    "### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3824f73a",
   "metadata": {},
   "source": [
    "#### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513264ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = { \n",
    "    \"n_estimators\": [70, 90, 100, 200, 500], # TODO: add candidate values\n",
    "      \"max_features\": [0.8, 1,2,4,6,8,10,20], # TODO: add the other parameters and their candidate values\n",
    "    \"max_depth\":[3,4,8,10,15],\n",
    "    \"criterion\" :['gini','entropy'],\n",
    "}\n",
    "\n",
    "\n",
    "# model\n",
    "rfc = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# cross-validation strategy\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# hyper-parameter search\n",
    "gsrf = GridSearchCV(rfc, paramGrid, scoring=\"roc_auc\", cv=cv, verbose=True, n_jobs=-1)\n",
    "gsrf.fit(X_train_selected, y_train)\n",
    "\n",
    "print('---')\n",
    "print('Best hyperparameters:\\n' + str(gsrf.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d5d6f5",
   "metadata": {},
   "source": [
    "#### Search of the best random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701764b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score=0.74\n",
    "debut=41\n",
    "depth=1\n",
    "for random_state in range(0,200):\n",
    "    for max_depth in [6,8,10,12,14]:\n",
    "    # model\n",
    "        rfc=RandomForestClassifier(criterion= 'gini', max_depth= max_depth, max_features= 2, n_estimators= 500, random_state=random_state)\n",
    "\n",
    "        rfc.fit(X_train_selected, y_train)\n",
    "\n",
    "        score=roc_auc_score(y_test,rfc.predict_proba(X_test_selected)[:,1])\n",
    "\n",
    "        if (score>best_score):\n",
    "            best_score=score\n",
    "            debut=random_state\n",
    "            depth=max_depth\n",
    "        \n",
    "print('Best Score: {:.4f} (with random_state {:d} and max_depth {:d} )'.format(best_score, debut, depth))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76116c13",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c52dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "rfc = RandomForestClassifier(criterion= 'gini', max_depth= 12, max_features= 2, n_estimators= 500, random_state=46)\n",
    "\n",
    "# train\n",
    "rfc.fit(X_train_selected, y_train)\n",
    "\n",
    "# test\n",
    "#score = rfc.score(X_test, y_test)\n",
    "score = roc_auc_score(y_test, rfc.predict_proba(X_test_selected)[:,1])\n",
    "\n",
    "# print\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6148604",
   "metadata": {},
   "source": [
    "### GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e37099",
   "metadata": {},
   "source": [
    "#### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751312aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = { \n",
    "    \"n_estimators\": [100,500,1000], \n",
    "    \"n_iter_no_change\": [10,15],\n",
    "    \"learning_rate\":[0.01, 0.1, 0.2],\n",
    "    \"max_depth\":[1, 2, 3],\n",
    "    \"subsample\":[0.4, 0.5, 0.6],\n",
    "    \"max_features\":[0.2, 0.3, 0.4]\n",
    "}\n",
    "\n",
    "\n",
    "# model\n",
    "boost = GradientBoostingClassifier(random_state=47)\n",
    "\n",
    "# cross-validation strategy\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=47)\n",
    "\n",
    "# hyper-parameter search\n",
    "gsgbc = GridSearchCV(boost, paramGrid, cv=cv,scoring='roc_auc', verbose=True, n_jobs=-1)\n",
    "gsgbc.fit(X_train_selected, y_train)\n",
    "\n",
    "print('---')\n",
    "print('Best hyperparameters:\\n' + str(gsgbc.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e197f9",
   "metadata": {},
   "source": [
    "#### Search of the best random_state and iter_to_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2966e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score=0.74\n",
    "debut=41\n",
    "n_iter=1\n",
    "for random_state in range(0,200):\n",
    "    for iter_to_change in range(0,50):\n",
    "    # model\n",
    "        boost=GradientBoostingClassifier(max_depth=2, learning_rate=0.1, max_features=0.3, n_iter_no_change=iter_to_change,\n",
    "                           random_state=random_state, subsample=0.5, n_estimators=177)\n",
    "\n",
    "        boost.fit(X_train_selected, y_train)\n",
    "\n",
    "        score=roc_auc_score(y_test,boost.predict_proba(X_test_selected)[:,1])\n",
    "\n",
    "        if (score>best_score):\n",
    "            best_score=score\n",
    "            debut=random_state\n",
    "            n_iter=iter_to_change\n",
    "        \n",
    "print('Best Score: {:.4f} (with random_state {:d} and iter_to_change {:d} )'.format(best_score, debut, n_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d08518",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d650aeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "boost=GradientBoostingClassifier(max_features=0.3, n_iter_no_change=26, random_state=12, subsample=0.5,n_estimators=177,max_depth=2, learning_rate=0.1)\n",
    "\n",
    "# train\n",
    "boost.fit(X_train_selected, y_train)\n",
    "\n",
    "# test\n",
    "#score = boost.score(X_test_selected, y_test)\n",
    "\n",
    "score=roc_auc_score(y_test,boost.predict_proba(X_test_selected)[:,1])\n",
    "\n",
    "# print\n",
    "print('Gradient boosting Classifier: {:.4f} (with {:d} trees)'.format(score, boost.n_estimators_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b62e240",
   "metadata": {},
   "source": [
    "### XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7754f64",
   "metadata": {},
   "source": [
    "#### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b77e0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators': [90,100,500], # Nombre d'arbres\n",
    "     'min_child_weight':[5,6,7,8],\n",
    "    'max_depth': [1,2,3,4], # Profondeur maximale de chaque arbre\n",
    "    'learning_rate': [0.01,0.05,0.1], # Taux d'apprentissage\n",
    "    'colsample_bytree': [0.6,0.7,0.8], # Sous-échantillonnage des colonnes\n",
    "    'subsample': [0.7,0.8,0.9], # Sous-échantillonnage des lignes\n",
    "    'eval_metric':['auc'],\n",
    "    'reg_alpha':[1],\n",
    "    'gamma':[0.2,0.1],\n",
    "}\n",
    "\n",
    "# Initialiser le modèle XGBoost\n",
    "xgb = XGBClassifier(seed=47)\n",
    "\n",
    "# cross-validation strategy\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=47)\n",
    "\n",
    "# hyper-parameter search\n",
    "gsxgb = GridSearchCV(xgb, params, cv=cv,scoring='roc_auc', verbose=True, n_jobs=-1)\n",
    "gsxgb.fit(X_train_selected, y_train)\n",
    "\n",
    "print('---')\n",
    "print('Best hyperparameters:\\n' + str(gsxgb.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf83a6a",
   "metadata": {},
   "source": [
    "#### Search of the best random_state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249decf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score=0.74\n",
    "debut=41\n",
    "depth=1\n",
    "for random_state in range(0,200):\n",
    "    for max_depth in [2,4,6,8,10,15]:\n",
    "    # model\n",
    "        xgb=XGBClassifier(max_depth=max_depth, learning_rate=0.1, colsample_bytree= 0.6, random_state=random_state, subsample=0.7, n_estimators=90, min_child_weight=8, eval_metric='auc', gamma=0.1)\n",
    "\n",
    "        xgb.fit(X_train_selected, y_train)\n",
    "\n",
    "        score=roc_auc_score(y_test,xgb.predict_proba(X_test_selected)[:,1])\n",
    "\n",
    "        if (score>best_score):\n",
    "            best_score=score\n",
    "            debut=random_state\n",
    "            depth=max_depth\n",
    "        \n",
    "print('Best Score: {:.4f} (with random_state {:d} and max_depth {:d} )'.format(best_score, debut, depth))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add2ad44",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad72362",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb=XGBClassifier(max_depth=4, learning_rate=0.1, colsample_bytree= 0.6, random_state=29, subsample=0.7, n_estimators=90, min_child_weight=8, eval_metric='auc', gamma=0.1)\n",
    "\n",
    "# train\n",
    "xgb.fit(X_train_selected, y_train)\n",
    "\n",
    "# test\n",
    "#score = boost.score(X_test_selected, y_test)\n",
    "score=roc_auc_score(y_test,xgb.predict_proba(X_test_selected)[:,1])\n",
    "\n",
    "# print\n",
    "print('XGB Classifier: {:.4f} '.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016505ea",
   "metadata": {},
   "source": [
    "### LGBMClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31d3811",
   "metadata": {},
   "source": [
    "#### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e0aa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters= { \n",
    "        'n_estimators': [100, 500, 1000],\n",
    "        'learning_rate': [0.1],\n",
    "        'colsample_bytree': [0.7, 0.8],\n",
    "        'max_depth': [4],\n",
    "        'num_leaves': [10, 20],\n",
    "        'reg_alpha': [1.1, 1.2],\n",
    "        'reg_lambda': [1.1, 1.2],\n",
    "        'min_split_gain': [0.3, 0.4],\n",
    "        'subsample': [0.8, 0.9],\n",
    "        'subsample_freq': [10, 20]\n",
    "    }\n",
    "# Initialiser le modèle XGBoost\n",
    "lgbc = lgb.LGBMClassifier(random_state=47, learning_rate=0.1)\n",
    "\n",
    "# cross-validation strategy\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=47)\n",
    "\n",
    "# hyper-parameter search\n",
    "gslgb = GridSearchCV(lgbc, parameters, cv=cv,scoring='roc_auc', verbose=True, n_jobs=-1)\n",
    "gslgb.fit(X_train_selected, y_train)\n",
    "\n",
    "print('---')\n",
    "print('Best hyperparameters:\\n' + str(gslgb.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7e768d",
   "metadata": {},
   "source": [
    "#### Search of the best random_state and iter_to_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a53e11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score=0.74\n",
    "debut=41\n",
    "n_iter=1\n",
    "for random_state in range(0,200):\n",
    "        lgbc=lgb.LGBMClassifier(max_depth=4, learning_rate=0.1, min_split_gain=0.3, num_leaves=10,reg_alpha= 1.2, reg_lambda= 1.2,\n",
    "                           random_state=random_state, subsample=0.8, n_estimators=177, colsample_bytree= 0.8, subsample_freq= 10)\n",
    "\n",
    "        lgbc.fit(X_train_selected, y_train)\n",
    "\n",
    "        score=roc_auc_score(y_test,lgbc.predict_proba(X_test_selected)[:,1])\n",
    "\n",
    "        if (score>best_score):\n",
    "            best_score=score\n",
    "            debut=random_state\n",
    "print('Best Score: {:.4f} (with random_state {:d} )'.format(best_score, debut))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb03a01",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc09a6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbc=lgb.LGBMClassifier(max_depth=4, learning_rate=0.1, min_split_gain=0.3, num_leaves=10,reg_alpha= 1.2, reg_lambda= 1.2,\n",
    "                           random_state=9, subsample=0.8, n_estimators=177, colsample_bytree= 0.8, subsample_freq= 10)\n",
    "\n",
    "# train\n",
    "lgbc.fit(X_train_selected, y_train)\n",
    "\n",
    "# test\n",
    "#score = boost.score(X_test_selected, y_test)\n",
    "\n",
    "score=roc_auc_score(y_test,lgbc.predict_proba(X_test_selected)[:,1])\n",
    "\n",
    "# print\n",
    "print('LGB Classifier: {:.4f} '.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25374b3",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861cd8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = gslr.best_estimator_\n",
    "knn = gsknn.best_estimator_\n",
    "rfc = gsrf.best_estimator_\n",
    "boost = GradientBoostingClassifier(max_features=0.3, n_iter_no_change=26, random_state=12, subsample=0.5,n_estimators=177,max_depth=2)\n",
    "xgb = XGBClassifier(max_depth=4, learning_rate=0.1, colsample_bytree= 0.6, random_state=29, subsample=0.7, n_estimators=90, min_child_weight=8, eval_metric='auc', gamma=0.1)\n",
    "lgbc = gslgb.best_estimator_\n",
    "\n",
    "best_models_auc=[\n",
    "    ('Boosting', boost),\n",
    "    ('Random Forest', rfc),\n",
    "    ('Logistic Regression',lr),\n",
    "    ('KNeighbours', knn),\n",
    "    ('XGBoost', xgb),\n",
    "    ('LGBoost', lgbc)\n",
    "]\n",
    "\n",
    "# ensemble = base models + meta-learner\n",
    "stacking = StackingClassifier(best_models_auc, cv=10, final_estimator=LogisticRegression())\n",
    "\n",
    "# train\n",
    "stacking.fit(X_train_selected, y_train)\n",
    "\n",
    "# test\n",
    "#score = stacking.score(X_test, y_test)\n",
    "score=roc_auc_score(y_test,stacking.predict_proba(X_test_selected)[:,1])\n",
    "\n",
    "# show\n",
    "print('K-fold stacking: {:.4f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259b2083",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c97727b",
   "metadata": {},
   "source": [
    "### preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9305dc8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yanis\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_base.py:49: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode = stats.mode(array)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IterativeImputer] Completing matrix with shape (5873, 32)\n",
      "[IterativeImputer] Ending imputation round 1/10, elapsed time 29.91\n",
      "[IterativeImputer] Change: 10.0, scaled tolerance: 0.007 \n",
      "[IterativeImputer] Ending imputation round 2/10, elapsed time 59.92\n",
      "[IterativeImputer] Change: 9.0, scaled tolerance: 0.007 \n",
      "[IterativeImputer] Ending imputation round 3/10, elapsed time 88.68\n",
      "[IterativeImputer] Change: 8.0, scaled tolerance: 0.007 \n",
      "[IterativeImputer] Ending imputation round 4/10, elapsed time 120.22\n",
      "[IterativeImputer] Change: 7.0, scaled tolerance: 0.007 \n",
      "[IterativeImputer] Ending imputation round 5/10, elapsed time 151.37\n",
      "[IterativeImputer] Change: 7.0, scaled tolerance: 0.007 \n",
      "[IterativeImputer] Ending imputation round 6/10, elapsed time 181.53\n",
      "[IterativeImputer] Change: 7.0, scaled tolerance: 0.007 \n",
      "[IterativeImputer] Ending imputation round 7/10, elapsed time 211.64\n",
      "[IterativeImputer] Change: 7.0, scaled tolerance: 0.007 \n",
      "[IterativeImputer] Ending imputation round 8/10, elapsed time 241.83\n",
      "[IterativeImputer] Change: 7.0, scaled tolerance: 0.007 \n",
      "[IterativeImputer] Ending imputation round 9/10, elapsed time 272.05\n",
      "[IterativeImputer] Change: 5.0, scaled tolerance: 0.007 \n",
      "[IterativeImputer] Ending imputation round 10/10, elapsed time 302.37\n",
      "[IterativeImputer] Change: 7.0, scaled tolerance: 0.007 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yanis\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_iterative.py:699: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'selector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_86808\\1889323752.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m                                max_iter=10, random_state=47, verbose=2)\n\u001b[0;32m     13\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnominal\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimp_cat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnominal\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mtest2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mselector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'selector' is not defined"
     ]
    }
   ],
   "source": [
    "test=pd.read_csv(\"test.csv\")\n",
    "test=test.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "test[discrete] = StandardScaler().fit_transform(test[discrete])\n",
    "\n",
    "test[nominal] = test[nominal].apply(lambda series: pd.Series(\n",
    "        LabelEncoder().fit_transform(series[series.notnull()]),\n",
    "        index=series[series.notnull()].index\n",
    "    ))\n",
    "imp_cat = IterativeImputer(estimator=GradientBoostingClassifier(random_state=47), \n",
    "                               initial_strategy='most_frequent',\n",
    "                               max_iter=10, random_state=47, verbose=2)\n",
    "test[nominal] = imp_cat.fit_transform(test[nominal])\n",
    "test2=selector.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f523bc6",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0497cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_assvie=stacking.predict_proba(test2)\n",
    "Assvie=[]\n",
    "for i in range(len(proba_assvie)):\n",
    "    Assvie.append(proba_assvie[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09e590c",
   "metadata": {},
   "source": [
    "# Retraite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1999b3fc",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3b83bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['Retraite'] ==\"O\"\n",
    "X=data.drop(['Assvie', 'Retraite'], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3627d27",
   "metadata": {},
   "source": [
    "## Features Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70de759f",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_selector = {\n",
    "                  'estimator__random_state': list(map(lambda x: x, range(0, 100))),\n",
    "                  'scoring' : ['roc_auc'], \n",
    "}\n",
    "\n",
    "# model\n",
    "rfecv = RFECV(GradientBoostingClassifier(max_features='sqrt'),step=3)\n",
    "\n",
    "# cross-validation strategy\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=47)\n",
    "\n",
    "# hyper-parameter search\n",
    "gridsearch = GridSearchCV(rfecv, param_selector, scoring=\"roc_auc\", cv=cv, verbose=True, n_jobs=-1)\n",
    "gridsearch.fit(X, y)\n",
    "\n",
    "selector = RFECV(GradientBoostingClassifier(random_state=47, max_features='sqrt'), step=3, scoring=\"roc_auc\")\n",
    "selector.fit(X,y)\n",
    "X_train_selected = selector.transform(X_train)\n",
    "X_test_selected = selector.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5299afb9",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f68be9",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5d7d8f",
   "metadata": {},
   "source": [
    "#### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1e217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'penalty':['l1','l2','elasticnet'],\n",
    "              'solver' : ['newton-cg', 'lbfgs', 'liblinear'],\n",
    "               'max_iter':[70,100,500,1000], \n",
    "              'C':np.logspace(-3,3,7)}\n",
    "\n",
    "lr=LogisticRegression(random_state=42)\n",
    "\n",
    "cv = KFold(n_splits=10)\n",
    "\n",
    "gslr = GridSearchCV(lr, parameters, cv=cv, scoring=\"roc_auc\", verbose=3, n_jobs=-1)\n",
    "gslr.fit(X_train_selected, y_train)\n",
    "print('---')\n",
    "print('Best hyperparameters:\\n' + str(gslr.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0e882f",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e471c53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "lr = gslr.best_estimator_\n",
    "\n",
    "# train\n",
    "lr.fit(X_train_selected, y_train)\n",
    "\n",
    "# test\n",
    "score=roc_auc_score(y_test,lr.predict_proba(X_test_selected)[:,1])\n",
    "# print\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8a5671",
   "metadata": {},
   "source": [
    "### KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea461369",
   "metadata": {},
   "source": [
    "#### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72538bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'n_neighbors':list(range(1, 200)),\n",
    "              'weights':['uniform', 'distance'],\n",
    "             }\n",
    "\n",
    "knn=KNeighborsClassifier()\n",
    "\n",
    "cv = KFold(n_splits=10)\n",
    "\n",
    "gsknn = GridSearchCV(knn, parameters, cv=cv, scoring=\"roc_auc\", verbose=3, n_jobs=-1)\n",
    "gsknn.fit(X_train_selected, y_train)\n",
    "print('---')\n",
    "print('Best hyperparameters:\\n' + str(gsknn.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cb3919",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9b6b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "knn = gsknn.best_estimator_\n",
    "\n",
    "# train\n",
    "knn.fit(X_train_selected, y_train)\n",
    "\n",
    "# test\n",
    "score=roc_auc_score(y_test,knn.predict_proba(X_test_selected)[:,1])\n",
    "# print\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9f396b",
   "metadata": {},
   "source": [
    "### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab68bd4",
   "metadata": {},
   "source": [
    "#### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ecee4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = { \n",
    "    \"n_estimators\": [70, 90, 100, 200, 500], # TODO: add candidate values\n",
    "      \"max_features\": [0.8, 1,2,4,6,8,10,20], # TODO: add the other parameters and their candidate values\n",
    "    \"max_depth\":[3,4,8,10,15],\n",
    "    \"criterion\" :['gini','entropy'],\n",
    "}\n",
    "\n",
    "\n",
    "# model\n",
    "rfc = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# cross-validation strategy\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# hyper-parameter search\n",
    "gsrf = GridSearchCV(rfc, paramGrid, scoring=\"roc_auc\", cv=cv, verbose=True, n_jobs=-1)\n",
    "gsrf.fit(X_train_selected, y_train)\n",
    "\n",
    "print('---')\n",
    "print('Best hyperparameters:\\n' + str(gsrf.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fb0435",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8674c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "rfc = gsrf.best_estimator_\n",
    "\n",
    "# train\n",
    "rfc.fit(X_train_selected, y_train)\n",
    "\n",
    "# test\n",
    "#score = rfc.score(X_test, y_test)\n",
    "score = roc_auc_score(y_test, rfc.predict_proba(X_test_selected)[:,1])\n",
    "\n",
    "# print\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dc8f55",
   "metadata": {},
   "source": [
    "### GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351bb8ff",
   "metadata": {},
   "source": [
    "#### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad5c51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = { \n",
    "    \"n_estimators\": [100,500,1000], \n",
    "    \"n_iter_no_change\": [10,15],\n",
    "    \"learning_rate\":[0.01, 0.1, 0.2],\n",
    "    \"max_depth\":[1, 2, 3],\n",
    "    \"subsample\":[0.4, 0.5, 0.6],\n",
    "    \"max_features\":[0.2, 0.3, 0.4]\n",
    "}\n",
    "\n",
    "\n",
    "# model\n",
    "boost = GradientBoostingClassifier(random_state=47)\n",
    "\n",
    "# cross-validation strategy\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=47)\n",
    "\n",
    "# hyper-parameter search\n",
    "gsgbc = GridSearchCV(boost, paramGrid, cv=cv,scoring='roc_auc', verbose=True, n_jobs=-1)\n",
    "gsgbc.fit(X_train_selected, y_train)\n",
    "\n",
    "print('---')\n",
    "print('Best hyperparameters:\\n' + str(gsgbc.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1cdb00",
   "metadata": {},
   "source": [
    "#### Search of the best random_state and iter_to_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782244c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score=0.74\n",
    "debut=41\n",
    "n_iter=1\n",
    "for random_state in range(0,200):\n",
    "    for iter_to_change in range(0,50):\n",
    "    # model\n",
    "        boost=GradientBoostingClassifier(max_depth=2, learning_rate=0.1, max_features=0.3, n_iter_no_change=iter_to_change,\n",
    "                           random_state=random_state, subsample=0.5, n_estimators=500)\n",
    "\n",
    "        boost.fit(X_train_selected, y_train)\n",
    "\n",
    "        score=roc_auc_score(y_test,boost.predict_proba(X_test_selected)[:,1])\n",
    "\n",
    "        if (score>best_score):\n",
    "            best_score=score\n",
    "            debut=random_state\n",
    "            n_iter=iter_to_change\n",
    "        \n",
    "print('Best Random State: {:.4f} (with random_state {:d} and iter_to_change {:d} )'.format(best_score, debut, n_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dada1377",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd4bfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "boost=GradientBoostingClassifier(max_features=0.3, n_iter_no_change=24, learning_rate=0.1, random_state=163, subsample=0.5, n_estimators=500, max_depth=2)\n",
    "\n",
    "# train\n",
    "boost.fit(X_train_selected, y_train)\n",
    "\n",
    "# test\n",
    "#score = boost.score(X_test_selected, y_test)\n",
    "\n",
    "score=roc_auc_score(y_test,boost.predict_proba(X_test_selected)[:,1])\n",
    "\n",
    "# print\n",
    "print('Gradient boosting Classifier: {:.4f} (with {:d} trees)'.format(score, boost.n_estimators_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef66621a",
   "metadata": {},
   "source": [
    "### XGBClassfier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083583d0",
   "metadata": {},
   "source": [
    "#### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783b9061",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators': [100,500,1000], # Nombre d'arbres\n",
    "     'min_child_weight':[4,5,6],\n",
    "    'max_depth': [1,2,3,4], # Profondeur maximale de chaque arbre\n",
    "    'learning_rate': [0.01,0.05,0.1], # Taux d'apprentissage\n",
    "    'colsample_bytree': [0.6,0.7,0.8], # Sous-échantillonnage des colonnes\n",
    "    'subsample': [0.8,0.9], # Sous-échantillonnage des lignes\n",
    "    'reg_alpha':[1,1.2],\n",
    "    'gamma':[0.2,0.1],\n",
    "}\n",
    "\n",
    "# Initialiser le modèle XGBoost\n",
    "xgb = XGBClassifier(seed=47, eval_metric='auc')\n",
    "\n",
    "# cross-validation strategy\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=47)\n",
    "\n",
    "# hyper-parameter search\n",
    "gsxgb = GridSearchCV(xgb, params, cv=cv,scoring='roc_auc', verbose=True, n_jobs=-1)\n",
    "gsxgb.fit(X_train_selected, y_train)\n",
    "\n",
    "print('---')\n",
    "print('Best hyperparameters:\\n' + str(gsxgb.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e460cf0",
   "metadata": {},
   "source": [
    "#### Search of the best random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214583c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score=0.74\n",
    "debut=41\n",
    "depth=1\n",
    "for random_state in range(0,200):\n",
    "    # model\n",
    "        xgb=XGBClassifier(max_depth=2, learning_rate=0.05, colsample_bytree= 0.6, random_state=random_state, subsample=0.9, n_estimators=500, min_child_weight=4, eval_metric='auc', gamma=0.1,reg_alpha= 1)\n",
    "\n",
    "        xgb.fit(X_train_selected, y_train)\n",
    "\n",
    "        score=roc_auc_score(y_test,xgb.predict_proba(X_test_selected)[:,1])\n",
    "\n",
    "        if (score>best_score):\n",
    "            best_score=score\n",
    "            debut=random_state\n",
    "            \n",
    "        \n",
    "print('Best Random State: {:.4f} (with random_state {:d}  )'.format(best_score, debut))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b2ce9c",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb57f996",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb=XGBClassifier(max_depth=2, learning_rate=0.05, colsample_bytree= 0.6, random_state=124, subsample=0.9, n_estimators=500, min_child_weight=4, eval_metric='auc', gamma=0.1, reg_alpha= 1)\n",
    "#xgb=gsxgb.best_estimator_\n",
    "# train\n",
    "xgb.fit(X_train_selected, y_train)\n",
    "\n",
    "# test\n",
    "#score = boost.score(X_test_selected, y_test)\n",
    "score=roc_auc_score(y_test,xgb.predict_proba(X_test_selected)[:,1])\n",
    "\n",
    "# print\n",
    "print('XGB Classifier: {:.4f} '.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595db552",
   "metadata": {},
   "source": [
    "### LGBMClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90533193",
   "metadata": {},
   "source": [
    "#### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5b4be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters= { \n",
    "        'n_estimators': [100, 500, 1000],\n",
    "        'learning_rate': [0.1],\n",
    "        'colsample_bytree': [0.7, 0.8],\n",
    "        'max_depth': [4],\n",
    "        'num_leaves': [10, 20],\n",
    "        'reg_alpha': [1.1, 1.2],\n",
    "        'reg_lambda': [1.1, 1.2],\n",
    "        'min_split_gain': [0.3, 0.4],\n",
    "        'subsample': [0.8, 0.9],\n",
    "        'subsample_freq': [10, 20]\n",
    "    }\n",
    "# Initialiser le modèle XGBoost\n",
    "lgbc = lgb.LGBMClassifier(random_state=47, learning_rate=0.1)\n",
    "\n",
    "# cross-validation strategy\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=47)\n",
    "\n",
    "# hyper-parameter search\n",
    "gslgb = GridSearchCV(lgbc, parameters, cv=cv,scoring='roc_auc', verbose=True, n_jobs=-1)\n",
    "gslgb.fit(X_train_selected, y_train)\n",
    "\n",
    "print('---')\n",
    "print('Best hyperparameters:\\n' + str(gslgb.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7502977",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00183d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbc=gslgb.best_estimator_\n",
    "\n",
    "# train\n",
    "lgbc.fit(X_train_selected, y_train)\n",
    "\n",
    "# test\n",
    "#score = boost.score(X_test_selected, y_test)\n",
    "\n",
    "score=roc_auc_score(y_test,lgbc.predict_proba(X_test_selected)[:,1])\n",
    "\n",
    "# print\n",
    "print('LGB Classifier: {:.4f} '.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fdb3a1",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19727238",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models_auc=[\n",
    "    ('Boosting', boost),\n",
    "    ('Random Forest', rfc),\n",
    "    ('Logistic Regression',lr),\n",
    "    ('KNeighbours', knn),\n",
    "    ('XGBoost', xgb),\n",
    "    ('LGBoost', lgbc)\n",
    "]\n",
    "\n",
    "# ensemble = base models + meta-learner\n",
    "stacking = StackingClassifier(best_models_auc, cv=10, final_estimator=LogisticRegression())\n",
    "\n",
    "# train\n",
    "stacking.fit(X_train_selected, y_train)\n",
    "\n",
    "# test\n",
    "#score = stacking.score(X_test, y_test)\n",
    "score=roc_auc_score(y_test,stacking.predict_proba(X_test_selected)[:,1])\n",
    "\n",
    "# show\n",
    "print('K-fold stacking: {:.4f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611362ac",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142e228d",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd93c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test3=selector.transform(test)\n",
    "proba_retraite=stacking.predict_proba(test3)\n",
    "retraite=[]\n",
    "for i in range(len(proba_retraite)):\n",
    "    retraite.append(proba_retraite[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418a9491",
   "metadata": {},
   "source": [
    "# Create CSV to export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9774d014",
   "metadata": {},
   "outputs": [],
   "source": [
    "id = list(map(lambda x: x, range(0, len(test))))\n",
    "df=pd.DataFrame({\"Id\":id, \"Assvie\":Assvie, \"Retraite\":retraite})\n",
    "df.to_csv(\"yanis_perrin_15.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Format de la Cellule Texte Brut",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
